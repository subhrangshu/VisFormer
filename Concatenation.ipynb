{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP2kbe2J2sw6"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LSOylaDvVVdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the Dataset\n",
        "import pandas as pd\n",
        "messages = pd.read_csv(\"/content/drive/MyDrive/INDOML Dataset/Datasets/nlp_vector.csv\")\n",
        "from transformers import pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
        "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
      ],
      "metadata": {
        "id": "L03qko7v28-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"),\n",
        "             Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "class TokenAndPositionEmbedding(Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "D4bQp4Zw2_yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def lament(a):\n",
        "    delimiters = '.',' ',',',';','!',':','?','\\t','\\n','\\0'\n",
        "    regex_pattern = '|'.join(map(re.escape, delimiters))\n",
        "    splits = re.split(regex_pattern, a)\n",
        "    lament = []\n",
        "    for i in splits:\n",
        "        lament.append(lemmatizer.lemmatize(i))\n",
        "    return ' '.join(lament)\n",
        "#messages['Lemmatized'] = messages.apply(lambda row : lament(row['Text']), axis = 1)\n",
        "messages"
      ],
      "metadata": {
        "id": "bpvOGeci3Eli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(a):\n",
        "  try:\n",
        "    return a.split()\n",
        "  except:\n",
        "    return [a]\n",
        "messages['Text'].fillna(0,inplace=True)\n",
        "messages['Listed'] = messages.apply(lambda row : split(row['Text']), axis = 1)\n",
        "messages"
      ],
      "metadata": {
        "id": "1D1P3iv-3GkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 200\n",
        "X = messages['Listed']\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=maxlen)\n",
        "y = messages['Label']\n",
        "X"
      ],
      "metadata": {
        "id": "mNNY7Dy63IbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "vocab_size=100000\n",
        "\n",
        "ann_inputs = Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(ann_inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(20, activation=\"relu\")(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(16, activation=\"softmax\")(x)\n",
        "\n",
        "ann_model = Model(inputs=ann_inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "vChHCRTd3Kzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESNET50**"
      ],
      "metadata": {
        "id": "GxgT_50n3TZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "#from keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "w0nZk1pZ3N90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# re-size all the images to this\n",
        "IMAGE_SIZE = [224,224]\n",
        "train_path = \"/content/drive/MyDrive/INDOML Dataset/Datasets/Train\""
      ],
      "metadata": {
        "id": "nY1jNLpa3W1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = ResNet50(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)"
      ],
      "metadata": {
        "id": "pDGmA26_3YlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# don't train existing weights\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "lez02Kw33bkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# our layers - you can add more if you want\n",
        "x = Flatten()(resnet.output)"
      ],
      "metadata": {
        "id": "vGrWB-wa3dL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # useful for getting number of output classes\n",
        "folders = glob(\"/content/drive/MyDrive/INDOML Dataset/Datasets/Train/*\")\n",
        "folders"
      ],
      "metadata": {
        "id": "2AWNYVkX3e5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = Dense(len(folders), activation='softmax')(x)\n",
        "# create a model object\n",
        "cnn_model = Model(inputs=resnet.input, outputs=prediction)"
      ],
      "metadata": {
        "id": "sGM-ONpn3g3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S25xh99TbNgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the Image Data Generator to import the images from the dataset\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ],
      "metadata": {
        "id": "jd0TJOcU5Ue0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure you provide the same target size as initialied for the image size\n",
        "training_set = train_datagen.flow_from_directory(\"/content/drive/MyDrive/INDOML Dataset/Datasets/Train\",\n",
        "                                                 target_size = (224,224),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical')"
      ],
      "metadata": {
        "id": "1gof_CoR5b02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Concatenate\n",
        "\n",
        "merged_model = Concatenate()([ann_model.output, Flatten()(resnet.output)])\n",
        "merged_model = Dense(32, activation='relu')(merged_model)\n",
        "merged_model = Dense(16, activation='softmax')(merged_model)"
      ],
      "metadata": {
        "id": "cshXXUFh3o_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = Model(inputs=[ann_model.input, resnet.input], outputs=merged_model)"
      ],
      "metadata": {
        "id": "6IGZz_qJ3ude"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a,b = training_set.next()"
      ],
      "metadata": {
        "id": "ENK1gNi_XNMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b6dbsbFHbtGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "final_model.fit([X_train, a], b, epochs=5)"
      ],
      "metadata": {
        "id": "ilcvIzz04s-T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}