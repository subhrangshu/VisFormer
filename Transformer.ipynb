{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic29qh_cHMBP"
      },
      "outputs": [],
      "source": [
        "# importing the Dataset\n",
        "import pandas as pd\n",
        "!pip install transformers\n",
        "messages = pd.read_csv(\"nlp_vector.csv\")\n",
        "from transformers import pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
        "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phGPFOXKnnqC"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"),\n",
        "             Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "class TokenAndPositionEmbedding(Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRDNLHo-trao"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def lament(a):\n",
        "    delimiters = '.',' ',',',';','!',':','?','\\t','\\n','\\0'\n",
        "    regex_pattern = '|'.join(map(re.escape, delimiters))\n",
        "    splits = re.split(regex_pattern, a)\n",
        "    lament = []\n",
        "    for i in splits:\n",
        "        lament.append(lemmatizer.lemmatize(i))\n",
        "    return ' '.join(lament)\n",
        "#messages['Lemmatized'] = messages.apply(lambda row : lament(row['Text']), axis = 1)\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "po8cIov2trdc"
      },
      "outputs": [],
      "source": [
        "def split(a):\n",
        "  try:\n",
        "    return a.split()\n",
        "  except:\n",
        "    return [a]\n",
        "messages['Text'].fillna(0,inplace=True)\n",
        "messages['Listed'] = messages.apply(lambda row : split(row['Text']), axis = 1)\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 200\n",
        "X = messages['Listed']\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=maxlen)\n",
        "y = messages['Label']\n",
        "X"
      ],
      "metadata": {
        "id": "8ZdGtFGXDAsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p1WYa0v8CUd"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "QmYyBhs1Ey2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb-7eYh-ol7J"
      },
      "outputs": [],
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "vocab_size=100000\n",
        "\n",
        "inputs = Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(20, activation=\"relu\")(x)\n",
        "x = Dropout(0.2)(x)\n",
        "outputs = Dense(16, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqWUDDfuol-C"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=64, epochs=200,\n",
        "                    validation_data=(X_test, y_test)\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hmr1pv9EomAm"
      },
      "outputs": [],
      "source": [
        "for name, value in zip(model.metrics_names, results):\n",
        "    print(\"%s: %.3f\" % (name, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKJTYp1jomFQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O1f34rQENLGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l305F26xNLJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SAtVbWiRNLLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7x6z3gfiNLN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
        "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"),\n",
        "             Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "class TokenAndPositionEmbedding(Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "yvnnvJSQNLQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC-X73oyomHc"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "maxlen = 200  # Only consider the first 200 words of each movie review\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = imdb.load_data(num_words=vocab_size)\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxMQ8iZHFiJ-"
      },
      "outputs": [],
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(20, activation=\"relu\")(x)\n",
        "x = Dropout(0.1)(x)\n",
        "outputs = Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Yiq_yaComJm"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=64, epochs=2,\n",
        "                    validation_data=(x_val, y_val)\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfPice2lomMb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47iDBJlTn6F7"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(y_train)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}