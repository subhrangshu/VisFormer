{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9biQqgguvfLm"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the Dataset\n",
        "import pandas as pd\n",
        "messages = pd.read_csv(\"/content/drive/MyDrive/INDOML Dataset/Datasets/INDOMLNLPTEXT.csv\")\n",
        "from transformers import pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
        "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
      ],
      "metadata": {
        "id": "smTqKY9Zvmej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"),\n",
        "             Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "class TokenAndPositionEmbedding(Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "pXaSHgBPvpFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "# download the required nltk resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# define a function to preprocess the text data\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses a given text string by tokenizing it into words,\n",
        "    removing stop words, and lowercasing all words.\n",
        "    \"\"\"\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    filtered_tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "# define a function to compute the word frequencies and their corresponding indices\n",
        "def compute_word_frequencies(messages):\n",
        "    \"\"\"\n",
        "    Computes the word frequencies and their corresponding indices\n",
        "    for all the messages in the given dataframe.\n",
        "    \"\"\"\n",
        "    word_counts = Counter()\n",
        "    for message in messages['Text']:\n",
        "        filtered_tokens = preprocess_text(message)\n",
        "        word_counts.update(filtered_tokens)\n",
        "    word_freqs = {word: idx for idx, (word, _) in enumerate(word_counts.most_common(), 1)}\n",
        "    return word_freqs\n",
        "\n",
        "# compute the word frequencies and their indices\n",
        "word_freqs = compute_word_frequencies(messages)\n",
        "\n",
        "# replace the words in the original dataset with their corresponding indices\n",
        "messages['Text'] = messages['Text'].apply(lambda message: ' '.join(str(word_freqs.get(word.lower(), 0)) for word in preprocess_text(message)))\n"
      ],
      "metadata": {
        "id": "pVyaIiWGEPtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "id": "RKSlAv3TEP1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages.to_csv('nlp_vector.csv', index=False)"
      ],
      "metadata": {
        "id": "_s5m0LosEP4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k-Gj9wUyEP6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5DSM9R8SEP9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ixUcIG4tEP_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N5xejpKwEQCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def lament(a):\n",
        "    delimiters = '.',' ',',',';','!',':','?','\\t','\\n','\\0'\n",
        "    regex_pattern = '|'.join(map(re.escape, delimiters))\n",
        "    splits = re.split(regex_pattern, a)\n",
        "    lament = []\n",
        "    for i in splits:\n",
        "        lament.append(lemmatizer.lemmatize(i))\n",
        "    return ' '.join(lament)\n",
        "messages['Lemmatized'] = messages.apply(lambda row : lament(row['Text']), axis = 1)\n",
        "messages"
      ],
      "metadata": {
        "id": "P-fdka8wvrQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str=\" \"\n",
        "for index, row in messages.iterrows():\n",
        "    str+=row['Lemmatized']"
      ],
      "metadata": {
        "id": "nyLYlgSa53yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "words = str.split()\n",
        "fdist1 = FreqDist(words)\n",
        "print(fdist1)\n",
        "print(fdist1.most_common())"
      ],
      "metadata": {
        "id": "2uquAN4l6wll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "maxlen = 2500\n",
        "cv = CountVectorizer(max_features=maxlen)\n",
        "X = pd.DataFrame(cv.fit_transform(messages['Lemmatized']).toarray())\n",
        "y = messages['Label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
      ],
      "metadata": {
        "id": "b_HTUsGEvuEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "Phq15AMWvvP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "iCpw4lmvvyfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "vocab_size=1278\n",
        "\n",
        "inputs = Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(20, activation=\"relu\")(x)\n",
        "x = Dropout(0.1)(x)\n",
        "outputs = Dense(16, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "UK1wJB9fv0AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=64, epochs=2,\n",
        "                    validation_data=(X_test, y_test)\n",
        "                   )"
      ],
      "metadata": {
        "id": "OhfgN8jGv4DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0R_kRNN83pfa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}